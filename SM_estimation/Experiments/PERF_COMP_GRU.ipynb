{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from tqdm import tqdm\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, output_size, n_feat, hidden_dim, n_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(n_feat, hidden_dim, n_layers)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden, batch_size):\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:17<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.3433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:34<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.3815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:34<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.3588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:50<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.3057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:39<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.3677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:03<01:31,  1.04it/s]"
     ]
    }
   ],
   "source": [
    "# Load Data \n",
    "cluster_range = 12\n",
    "seq_len = 12\n",
    "lstm_mse = []\n",
    "epochs = 100\n",
    "data_path = '/Users/muneeza/Documents/GitHub/DATA_SMest/'\n",
    "\n",
    "# Set description variables\n",
    "sub_feat_names = ['AREA', 'PRECIP' , 'ET', 'SW_END', 'PERC', 'GW_RCHG', 'DA_RCHG', 'REVAP', 'SA_IRR', 'DA_IRR', 'SA_ST', 'DA_ST',\n",
    "                 'WYLD', 'DAILYCN', 'TMP_AV', 'SOL_TMP', 'SOLAR']\n",
    "\n",
    "train_id_en = 27*12\n",
    "test_id_en = 34*12 \n",
    "\n",
    "for cluster in range(cluster_range):\n",
    "    # Read data for the cluster -  (timeseries, hrus , features)\n",
    "    data = np.load(data_path+'sub_hru_sub_data_clstr_'+str(cluster)+'.npy') # (tstep , hrus, features)\n",
    "\n",
    "    # Select id for target variable \n",
    "    target_id = sub_feat_names.index('SW_END')\n",
    "\n",
    "\n",
    "    # Test Train Split \n",
    "    X_train = data[ 0:train_id_en, : ,  :]\n",
    "    X_test = data[ train_id_en: test_id_en , : ,  :]\n",
    "\n",
    "    # Normalize data\n",
    "    X_train = TimeSeriesScalerMinMax(value_range=(-1,1)).fit_transform( np.transpose(X_train,(1,0,2)) )\n",
    "    X_test = TimeSeriesScalerMinMax(value_range=(-1,1)).fit_transform( np.transpose(X_test,(1,0,2)) )\n",
    "    X_train = np.transpose(X_train, (1,0,2) ) # reshape back to original \n",
    "    X_test = np.transpose(X_test, (1,0,2) ) # reshape back to original \n",
    "\n",
    "\n",
    "    # Convert to supervised learneing example, target is t+1\n",
    "\n",
    "    n_feat = len(sub_feat_names)\n",
    "    n_hrus = X_train.shape[1]\n",
    "    test_len = X_test.shape[0]\n",
    "\n",
    "    Y_train = np.zeros((train_id_en - seq_len , n_hrus, seq_len))\n",
    "    Y_test = np.zeros((test_len - seq_len , n_hrus, seq_len))\n",
    "\n",
    "    for i in range(1 , train_id_en - seq_len):\n",
    "        Y_train[i, : , :] = X_train[i:i+seq_len  , : , target_id ].T\n",
    "\n",
    "    for i in range(1 , test_len - seq_len):    \n",
    "        Y_test[i, : , :] = X_test[i:i+seq_len , : , target_id].T\n",
    "        \n",
    "\n",
    "    X_train = X_train[0:train_id_en-seq_len , : , :  ]\n",
    "    X_test = X_test[0:test_len-seq_len , : , : ]\n",
    "\n",
    "    # Set up model for training \n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = LSTM(output_size=12 , n_feat= n_feat, hidden_dim=32, n_layers=1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    model.train()\n",
    "    hist = []\n",
    "    train_dataset = torch.tensor(X_train).double()\n",
    "    model = model.double()\n",
    "    batch_size = X_train.shape[1]\n",
    "    for ep in tqdm(range(epochs)):\n",
    "        cost = torch.tensor([0.0],requires_grad=True)\n",
    "        hidden = model.init_hidden(batch_size = batch_size)\n",
    "        y_hat = model(train_dataset, hidden, batch_size=batch_size)\n",
    "        cost = cost + torch.mean((y_hat[0].data-Y_train)**2)\n",
    "        cost = cost / (ep+1)\n",
    "        hist.append(cost.item())\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Performance on test data  \n",
    "    model.eval()\n",
    "    test_dataset = torch.tensor(X_test).double()\n",
    "    cost = 0\n",
    "    batch_size = test_dataset.shape[1]\n",
    "    hidden = model.init_hidden(batch_size = batch_size)\n",
    "    y_hat = model(test_dataset, hidden, batch_size=batch_size)\n",
    "    cost = cost + torch.mean((y_hat[0].data-Y_test)**2)\n",
    "    lstm_mse.append(cost.item())\n",
    "    print(\"MSE: {:.4f}\".format(lstm_mse[cluster]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
